{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "from sklearn import preprocessing\n",
    "np.random.seed(1337)\n",
    "\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import LSTM\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read started...\n",
      "Data read finished.\n",
      "(713, 19)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data read started...\")\n",
    "data = pd.read_csv(\"nback/result8.csv\")\n",
    "data = data.as_matrix()\n",
    "print (\"Data read finished.\")\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminate EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero: 138  one:  133  two:  142  three:  300\n"
     ]
    }
   ],
   "source": [
    "for i in range (1,5):\n",
    "    data = np.delete(data, 1, 1) \n",
    "    \n",
    "#data = np.delete(data, 1, 1) \n",
    "data = data[:,0:7]\n",
    "data.shape\n",
    "\n",
    "data[0]\n",
    "\n",
    "zero = 0\n",
    "one = 0\n",
    "two = 0\n",
    "three = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if (data[i, 0] == '0back'):\n",
    "        zero = zero + 1\n",
    "    if (data[i, 0] == '1back'):\n",
    "        one = one + 1\n",
    "    if (data[i, 0] == '2back'):\n",
    "        two = two + 1\n",
    "    if (data[i, 0] == '3back'):\n",
    "        three = three + 1\n",
    "\n",
    "print('zero:', zero, ' one: ', one, ' two: ', two, ' three: ', three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary for the levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "level=[\"0back\",\"1back\",\"2back\",\"3back\"]\n",
    "level2int = dict((p, i) for i, p in enumerate(level))\n",
    "int2level = dict((i, p) for i, p in enumerate(level))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/amoro/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "for i in range (1,7):\n",
    "    data[:, i] = preprocessing.scale(data[:, i])\n",
    "\n",
    "back0 = np.zeros((zero,6))\n",
    "back1 = np.zeros((one,6))\n",
    "back2 = np.zeros((two,6))\n",
    "back3 = np.zeros((three,6))\n",
    "j = 0\n",
    "k = 0\n",
    "l = 0\n",
    "m = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if (data[i, 0] == '0back'):\n",
    "        back0[j] = data[i, 1:]\n",
    "        j = j+1\n",
    "    if (data[i, 0] == '1back'):\n",
    "        back1[k] = data[i, 1:]\n",
    "        k = k+1\n",
    "    if (data[i, 0] == '2back'):\n",
    "        back2[l] = data[i, 1:]\n",
    "        l = l+1\n",
    "    if (data[i, 0] == '3back'):\n",
    "        back3[m] = data[i, 1:]\n",
    "        m = m+1\n",
    "\n",
    "y_data = data[:, 0]\n",
    "for i in range(len(data)):\n",
    "    y_data[i] = level2int[y_data[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "one_hot = ohe.fit_transform(y_data.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group data for recurrent network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window = 8\n",
    "null = np.array([0,0,0,0,0,0])\n",
    "\n",
    "for i in range(window-1):\n",
    "        back0 = np.vstack([back0, null])\n",
    "        back1 = np.vstack([back1, null])\n",
    "        back2 = np.vstack([back2, null])\n",
    "        back3 = np.vstack([back3, null])\n",
    "        \n",
    "for i in range(window-1):\n",
    "        back0 = np.vstack([null, back0])\n",
    "        back1 = np.vstack([null, back1])\n",
    "        back2 = np.vstack([null, back2])\n",
    "        back3 = np.vstack([null, back3])\n",
    "\n",
    "data_length = len(back0) + window - 1 + len(back1) + window - 1 + len(back2) + window - 1 + len(back3) + window - 1\n",
    "\n",
    "x_data = np.zeros((data_length,window, 6))         # final input data for the network\n",
    "y_one_hot = np.zeros((data_length,4))             # to store one-hot data groupped\n",
    "\n",
    "\n",
    "for i in range(len(back0)-window):\n",
    "    for j in range(window):\n",
    "        x_data[i, j, :] = back0[i+j]\n",
    "    y_one_hot[i] = [0,0,0,1]\n",
    "    \n",
    "\n",
    "index = 0\n",
    "for i in range(len(back0)-window, len(back0)-window+ len(back1)-window):\n",
    "    for j in range(window):\n",
    "        x_data[i, j, :] = back1[index+j]\n",
    "    y_one_hot[i] = [0,0,1,0]\n",
    "    index = index+1\n",
    "\n",
    "index = 0\n",
    "for i in range(len(back0)-window+ len(back1)-window, len(back0)-window + len(back1)-window + len(back2)-window):\n",
    "    for j in range(window):\n",
    "        x_data[i, j, :] = back2[index+j]\n",
    "    y_one_hot[i] = [0,1,0,0]\n",
    "        \n",
    "    index = index+1\n",
    "    \n",
    "\n",
    "index = 0\n",
    "for i in range(len(back0)-window+ len(back1)-window + len(back2)-window, \n",
    "                   len(back0)-window + len(back1)-window + len(back2)-window + len(back3)-window):\n",
    "    for j in range(window):\n",
    "        x_data[i, j, :] = back3[index+j]\n",
    "    y_one_hot[i] = [1,0,0,0]\n",
    "    index = index+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(x_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "x_data = x_data[indices]\n",
    "y_one_hot = y_one_hot[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide into train, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(557, 8, 6) (557, 4) (119, 8, 6) (119, 4) (121, 8, 6) (121, 4)\n"
     ]
    }
   ],
   "source": [
    "len_data = len(x_data)\n",
    "\n",
    "nb_test = int(len_data*0.15)\n",
    "nb_validation = int(len_data*0.15)\n",
    "nb_train = int(len_data*0.7)\n",
    "\n",
    "end_valid = nb_train+nb_validation\n",
    "\n",
    "x_train = x_data[0:nb_train]\n",
    "y_train = y_one_hot[0:nb_train]\n",
    "\n",
    "x_valid = x_data[nb_train:end_valid]\n",
    "y_valid = y_one_hot[nb_train:end_valid]\n",
    "\n",
    "x_test = x_data[end_valid:]\n",
    "y_test = y_one_hot[end_valid:]\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_valid.shape, y_valid.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "earlyStopping=keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=0, mode='auto')\n",
    "\n",
    "model.add(LSTM(64, input_dim=6, input_length=window, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "    \n",
    "\n",
    "model.add(LSTM(32,return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(output_dim=y_one_hot.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 557 samples, validate on 119 samples\n",
      "Epoch 1/200\n",
      "557/557 [==============================] - 2s - loss: 1.2215 - acc: 0.4129 - val_loss: 1.0401 - val_acc: 0.7647\n",
      "Epoch 2/200\n",
      "557/557 [==============================] - 0s - loss: 1.0423 - acc: 0.6355 - val_loss: 0.8806 - val_acc: 0.7731\n",
      "Epoch 3/200\n",
      "557/557 [==============================] - 0s - loss: 0.9186 - acc: 0.6553 - val_loss: 0.7664 - val_acc: 0.7731\n",
      "Epoch 4/200\n",
      "557/557 [==============================] - 0s - loss: 0.8094 - acc: 0.6984 - val_loss: 0.6750 - val_acc: 0.7815\n",
      "Epoch 5/200\n",
      "557/557 [==============================] - 0s - loss: 0.7313 - acc: 0.7307 - val_loss: 0.6159 - val_acc: 0.7815\n",
      "Epoch 6/200\n",
      "557/557 [==============================] - 0s - loss: 0.6717 - acc: 0.7469 - val_loss: 0.5809 - val_acc: 0.7899\n",
      "Epoch 7/200\n",
      "557/557 [==============================] - 0s - loss: 0.6453 - acc: 0.7271 - val_loss: 0.5496 - val_acc: 0.7815\n",
      "Epoch 8/200\n",
      "557/557 [==============================] - 0s - loss: 0.6381 - acc: 0.7469 - val_loss: 0.5305 - val_acc: 0.7983\n",
      "Epoch 9/200\n",
      "557/557 [==============================] - 0s - loss: 0.5907 - acc: 0.7540 - val_loss: 0.5175 - val_acc: 0.7983\n",
      "Epoch 10/200\n",
      "557/557 [==============================] - 0s - loss: 0.5972 - acc: 0.7504 - val_loss: 0.5039 - val_acc: 0.8067\n",
      "Epoch 11/200\n",
      "557/557 [==============================] - 0s - loss: 0.5688 - acc: 0.7630 - val_loss: 0.4883 - val_acc: 0.8319\n",
      "Epoch 12/200\n",
      "557/557 [==============================] - 0s - loss: 0.5602 - acc: 0.7612 - val_loss: 0.4719 - val_acc: 0.8655\n",
      "Epoch 13/200\n",
      "557/557 [==============================] - 0s - loss: 0.5181 - acc: 0.7864 - val_loss: 0.4704 - val_acc: 0.8319\n",
      "Epoch 14/200\n",
      "557/557 [==============================] - 0s - loss: 0.4952 - acc: 0.7882 - val_loss: 0.4455 - val_acc: 0.8655\n",
      "Epoch 15/200\n",
      "557/557 [==============================] - 0s - loss: 0.4850 - acc: 0.8007 - val_loss: 0.4249 - val_acc: 0.8655\n",
      "Epoch 16/200\n",
      "557/557 [==============================] - 0s - loss: 0.4917 - acc: 0.7917 - val_loss: 0.4249 - val_acc: 0.8655\n",
      "Epoch 17/200\n",
      "557/557 [==============================] - 0s - loss: 0.4662 - acc: 0.8223 - val_loss: 0.4366 - val_acc: 0.8571\n",
      "Epoch 18/200\n",
      "557/557 [==============================] - 0s - loss: 0.4214 - acc: 0.8330 - val_loss: 0.4022 - val_acc: 0.8571\n",
      "Epoch 19/200\n",
      "557/557 [==============================] - 0s - loss: 0.4227 - acc: 0.8366 - val_loss: 0.4222 - val_acc: 0.8571\n",
      "Epoch 20/200\n",
      "557/557 [==============================] - 0s - loss: 0.4111 - acc: 0.8420 - val_loss: 0.4049 - val_acc: 0.8655\n",
      "Epoch 21/200\n",
      "557/557 [==============================] - 0s - loss: 0.3953 - acc: 0.8438 - val_loss: 0.4027 - val_acc: 0.8739\n",
      "Epoch 22/200\n",
      "557/557 [==============================] - 0s - loss: 0.3805 - acc: 0.8474 - val_loss: 0.3882 - val_acc: 0.8824\n",
      "Epoch 23/200\n",
      "557/557 [==============================] - 0s - loss: 0.3870 - acc: 0.8438 - val_loss: 0.3686 - val_acc: 0.8739\n",
      "Epoch 24/200\n",
      "557/557 [==============================] - 0s - loss: 0.3635 - acc: 0.8420 - val_loss: 0.3688 - val_acc: 0.8908\n",
      "Epoch 25/200\n",
      "557/557 [==============================] - 0s - loss: 0.3454 - acc: 0.8636 - val_loss: 0.3321 - val_acc: 0.8739\n",
      "Epoch 26/200\n",
      "557/557 [==============================] - 0s - loss: 0.3606 - acc: 0.8582 - val_loss: 0.3555 - val_acc: 0.8739\n",
      "Epoch 27/200\n",
      "557/557 [==============================] - 0s - loss: 0.3749 - acc: 0.8654 - val_loss: 0.3501 - val_acc: 0.8319\n",
      "Epoch 28/200\n",
      "557/557 [==============================] - 0s - loss: 0.3234 - acc: 0.8707 - val_loss: 0.3062 - val_acc: 0.8739\n",
      "Epoch 29/200\n",
      "557/557 [==============================] - 0s - loss: 0.3074 - acc: 0.8654 - val_loss: 0.3105 - val_acc: 0.8824\n",
      "Epoch 30/200\n",
      "557/557 [==============================] - 0s - loss: 0.3147 - acc: 0.8743 - val_loss: 0.3160 - val_acc: 0.8403\n",
      "Epoch 31/200\n",
      "557/557 [==============================] - 0s - loss: 0.2951 - acc: 0.8815 - val_loss: 0.2964 - val_acc: 0.9076\n",
      "Epoch 32/200\n",
      "557/557 [==============================] - 0s - loss: 0.2803 - acc: 0.8869 - val_loss: 0.2831 - val_acc: 0.8824\n",
      "Epoch 33/200\n",
      "557/557 [==============================] - 0s - loss: 0.2767 - acc: 0.8797 - val_loss: 0.2519 - val_acc: 0.9244\n",
      "Epoch 34/200\n",
      "557/557 [==============================] - 0s - loss: 0.2933 - acc: 0.8564 - val_loss: 0.2577 - val_acc: 0.8487\n",
      "Epoch 35/200\n",
      "557/557 [==============================] - 0s - loss: 0.2557 - acc: 0.8797 - val_loss: 0.2593 - val_acc: 0.9160\n",
      "Epoch 36/200\n",
      "557/557 [==============================] - 0s - loss: 0.2453 - acc: 0.8869 - val_loss: 0.2761 - val_acc: 0.8908\n",
      "Epoch 37/200\n",
      "557/557 [==============================] - 0s - loss: 0.2546 - acc: 0.8959 - val_loss: 0.2409 - val_acc: 0.9244\n",
      "Epoch 38/200\n",
      "557/557 [==============================] - 0s - loss: 0.2584 - acc: 0.9066 - val_loss: 0.2457 - val_acc: 0.8824\n",
      "Epoch 39/200\n",
      "557/557 [==============================] - 0s - loss: 0.2559 - acc: 0.8869 - val_loss: 0.2392 - val_acc: 0.9244\n",
      "Epoch 40/200\n",
      "557/557 [==============================] - 0s - loss: 0.2870 - acc: 0.8492 - val_loss: 0.2972 - val_acc: 0.8571\n",
      "Epoch 41/200\n",
      "557/557 [==============================] - 0s - loss: 0.2495 - acc: 0.8761 - val_loss: 0.2353 - val_acc: 0.9076\n",
      "Epoch 42/200\n",
      "557/557 [==============================] - 0s - loss: 0.2730 - acc: 0.8743 - val_loss: 0.2600 - val_acc: 0.8824\n",
      "Epoch 43/200\n",
      "557/557 [==============================] - 0s - loss: 0.2401 - acc: 0.8941 - val_loss: 0.2512 - val_acc: 0.9244\n",
      "Epoch 44/200\n",
      "557/557 [==============================] - 1s - loss: 0.2307 - acc: 0.9084 - val_loss: 0.2295 - val_acc: 0.9244\n",
      "Epoch 45/200\n",
      "557/557 [==============================] - 1s - loss: 0.2661 - acc: 0.8654 - val_loss: 0.2628 - val_acc: 0.8908\n",
      "Epoch 46/200\n",
      "557/557 [==============================] - 1s - loss: 0.2385 - acc: 0.8905 - val_loss: 0.2698 - val_acc: 0.8739\n",
      "Epoch 47/200\n",
      "557/557 [==============================] - 1s - loss: 0.2323 - acc: 0.8671 - val_loss: 0.2115 - val_acc: 0.9328\n",
      "Epoch 48/200\n",
      "557/557 [==============================] - 0s - loss: 0.2624 - acc: 0.8636 - val_loss: 0.2375 - val_acc: 0.8992\n",
      "Epoch 49/200\n",
      "557/557 [==============================] - 0s - loss: 0.2290 - acc: 0.8671 - val_loss: 0.2071 - val_acc: 0.9160\n",
      "Epoch 50/200\n",
      "557/557 [==============================] - 0s - loss: 0.2154 - acc: 0.8636 - val_loss: 0.2048 - val_acc: 0.9412\n",
      "Epoch 51/200\n",
      "557/557 [==============================] - 0s - loss: 0.1984 - acc: 0.8905 - val_loss: 0.2220 - val_acc: 0.8403\n",
      "Epoch 52/200\n",
      "557/557 [==============================] - 0s - loss: 0.2086 - acc: 0.8707 - val_loss: 0.2181 - val_acc: 0.8655\n",
      "Epoch 53/200\n",
      "557/557 [==============================] - 0s - loss: 0.1977 - acc: 0.8761 - val_loss: 0.1994 - val_acc: 0.8739\n",
      "Epoch 54/200\n",
      "557/557 [==============================] - 0s - loss: 0.1797 - acc: 0.8689 - val_loss: 0.1862 - val_acc: 0.8992\n",
      "Epoch 55/200\n",
      "557/557 [==============================] - 0s - loss: 0.1854 - acc: 0.8492 - val_loss: 0.1823 - val_acc: 0.8824\n",
      "Epoch 56/200\n",
      "557/557 [==============================] - 0s - loss: 0.1710 - acc: 0.8797 - val_loss: 0.1865 - val_acc: 0.8908\n",
      "Epoch 57/200\n",
      "557/557 [==============================] - 0s - loss: 0.1851 - acc: 0.8959 - val_loss: 0.1926 - val_acc: 0.8824\n",
      "Epoch 58/200\n",
      "557/557 [==============================] - 0s - loss: 0.1681 - acc: 0.8600 - val_loss: 0.2075 - val_acc: 0.8739\n",
      "Epoch 59/200\n",
      "557/557 [==============================] - 0s - loss: 0.1683 - acc: 0.8707 - val_loss: 0.1908 - val_acc: 0.8908\n",
      "Epoch 60/200\n",
      "557/557 [==============================] - 0s - loss: 0.1616 - acc: 0.8887 - val_loss: 0.1797 - val_acc: 0.8739\n",
      "Epoch 61/200\n",
      "557/557 [==============================] - 0s - loss: 0.1777 - acc: 0.8689 - val_loss: 0.1788 - val_acc: 0.8739\n",
      "Epoch 62/200\n",
      "557/557 [==============================] - 0s - loss: 0.1746 - acc: 0.8546 - val_loss: 0.1759 - val_acc: 0.8824\n",
      "Epoch 63/200\n",
      "557/557 [==============================] - 0s - loss: 0.1673 - acc: 0.8654 - val_loss: 0.1750 - val_acc: 0.8908\n",
      "Epoch 64/200\n",
      "557/557 [==============================] - 0s - loss: 0.1826 - acc: 0.8654 - val_loss: 0.1704 - val_acc: 0.8992\n",
      "Epoch 65/200\n",
      "557/557 [==============================] - 0s - loss: 0.1907 - acc: 0.8510 - val_loss: 0.1861 - val_acc: 0.8908\n",
      "Epoch 66/200\n",
      "557/557 [==============================] - 0s - loss: 0.1783 - acc: 0.8743 - val_loss: 0.1887 - val_acc: 0.8655\n",
      "Epoch 67/200\n",
      "557/557 [==============================] - 0s - loss: 0.1884 - acc: 0.8797 - val_loss: 0.1796 - val_acc: 0.8908\n",
      "Epoch 68/200\n",
      "557/557 [==============================] - 0s - loss: 0.1635 - acc: 0.8815 - val_loss: 0.1753 - val_acc: 0.8824\n",
      "Epoch 69/200\n",
      "557/557 [==============================] - 0s - loss: 0.1324 - acc: 0.8995 - val_loss: 0.1650 - val_acc: 0.8908\n",
      "Epoch 70/200\n",
      "557/557 [==============================] - 0s - loss: 0.1306 - acc: 0.8923 - val_loss: 0.1813 - val_acc: 0.8739\n",
      "Epoch 71/200\n",
      "557/557 [==============================] - 0s - loss: 0.1379 - acc: 0.8815 - val_loss: 0.1682 - val_acc: 0.8908\n",
      "Epoch 72/200\n",
      "557/557 [==============================] - 0s - loss: 0.1618 - acc: 0.8743 - val_loss: 0.1782 - val_acc: 0.8824\n",
      "Epoch 73/200\n",
      "557/557 [==============================] - 0s - loss: 0.1360 - acc: 0.8671 - val_loss: 0.1514 - val_acc: 0.8992\n",
      "Epoch 74/200\n",
      "557/557 [==============================] - 0s - loss: 0.1252 - acc: 0.8797 - val_loss: 0.1655 - val_acc: 0.8824\n",
      "Epoch 75/200\n",
      "557/557 [==============================] - 0s - loss: 0.1330 - acc: 0.8887 - val_loss: 0.1638 - val_acc: 0.8992\n",
      "Epoch 76/200\n",
      "557/557 [==============================] - 0s - loss: 0.1226 - acc: 0.8923 - val_loss: 0.1790 - val_acc: 0.8908\n",
      "Epoch 77/200\n",
      "557/557 [==============================] - 0s - loss: 0.1378 - acc: 0.8815 - val_loss: 0.1659 - val_acc: 0.8908\n",
      "Epoch 78/200\n",
      "557/557 [==============================] - 0s - loss: 0.1366 - acc: 0.8869 - val_loss: 0.1524 - val_acc: 0.8908\n",
      "Epoch 79/200\n",
      "557/557 [==============================] - 0s - loss: 0.1363 - acc: 0.8869 - val_loss: 0.1639 - val_acc: 0.9076\n",
      "Epoch 80/200\n",
      "557/557 [==============================] - 0s - loss: 0.2213 - acc: 0.8671 - val_loss: 0.1945 - val_acc: 0.8739\n",
      "Epoch 81/200\n",
      "557/557 [==============================] - 0s - loss: 0.1735 - acc: 0.8779 - val_loss: 0.1563 - val_acc: 0.8992\n",
      "Epoch 82/200\n",
      "557/557 [==============================] - 0s - loss: 0.1557 - acc: 0.8743 - val_loss: 0.1633 - val_acc: 0.8824\n",
      "Epoch 83/200\n",
      "557/557 [==============================] - 0s - loss: 0.1351 - acc: 0.8779 - val_loss: 0.1724 - val_acc: 0.8824\n",
      "Epoch 84/200\n",
      "557/557 [==============================] - 0s - loss: 0.1569 - acc: 0.8654 - val_loss: 0.1783 - val_acc: 0.8739\n",
      "Epoch 85/200\n",
      "557/557 [==============================] - 0s - loss: 0.1372 - acc: 0.8869 - val_loss: 0.1644 - val_acc: 0.8908\n",
      "Epoch 86/200\n",
      "557/557 [==============================] - 0s - loss: 0.1374 - acc: 0.8815 - val_loss: 0.1736 - val_acc: 0.8824\n",
      "Epoch 87/200\n",
      "557/557 [==============================] - 0s - loss: 0.1354 - acc: 0.8707 - val_loss: 0.1438 - val_acc: 0.9076\n",
      "Epoch 88/200\n",
      "557/557 [==============================] - 0s - loss: 0.1194 - acc: 0.8797 - val_loss: 0.1458 - val_acc: 0.8992\n",
      "Epoch 89/200\n",
      "557/557 [==============================] - 0s - loss: 0.1330 - acc: 0.8815 - val_loss: 0.1613 - val_acc: 0.8908\n",
      "Epoch 90/200\n",
      "557/557 [==============================] - 0s - loss: 0.0942 - acc: 0.8959 - val_loss: 0.1592 - val_acc: 0.8992\n",
      "Epoch 91/200\n",
      "557/557 [==============================] - 0s - loss: 0.1221 - acc: 0.8815 - val_loss: 0.1669 - val_acc: 0.8992\n",
      "Epoch 92/200\n",
      "557/557 [==============================] - 0s - loss: 0.1128 - acc: 0.8905 - val_loss: 0.1467 - val_acc: 0.8908\n",
      "Epoch 93/200\n",
      "557/557 [==============================] - 0s - loss: 0.1047 - acc: 0.8815 - val_loss: 0.2021 - val_acc: 0.8908\n",
      "Epoch 94/200\n",
      "557/557 [==============================] - 0s - loss: 0.1121 - acc: 0.8887 - val_loss: 0.1467 - val_acc: 0.8908\n",
      "Epoch 95/200\n",
      "557/557 [==============================] - 0s - loss: 0.1123 - acc: 0.8959 - val_loss: 0.1403 - val_acc: 0.8992\n",
      "Epoch 96/200\n",
      "557/557 [==============================] - 0s - loss: 0.1170 - acc: 0.8779 - val_loss: 0.1421 - val_acc: 0.9076\n",
      "Epoch 97/200\n",
      "557/557 [==============================] - 0s - loss: 0.1053 - acc: 0.8869 - val_loss: 0.1614 - val_acc: 0.8908\n",
      "Epoch 98/200\n",
      "557/557 [==============================] - 0s - loss: 0.0871 - acc: 0.8995 - val_loss: 0.1519 - val_acc: 0.8992\n",
      "Epoch 99/200\n",
      "557/557 [==============================] - 0s - loss: 0.1108 - acc: 0.8779 - val_loss: 0.1710 - val_acc: 0.8992\n",
      "Epoch 100/200\n",
      "557/557 [==============================] - 0s - loss: 0.1128 - acc: 0.8887 - val_loss: 0.1677 - val_acc: 0.8908\n",
      "Epoch 101/200\n",
      "557/557 [==============================] - 0s - loss: 0.1036 - acc: 0.8887 - val_loss: 0.1418 - val_acc: 0.8992\n",
      "Epoch 102/200\n",
      "557/557 [==============================] - 0s - loss: 0.1211 - acc: 0.8815 - val_loss: 0.1631 - val_acc: 0.8908\n",
      "Epoch 103/200\n",
      "557/557 [==============================] - 0s - loss: 0.1368 - acc: 0.8707 - val_loss: 0.1598 - val_acc: 0.8908\n",
      "Epoch 104/200\n",
      "557/557 [==============================] - 0s - loss: 0.1341 - acc: 0.8743 - val_loss: 0.1537 - val_acc: 0.8908\n",
      "Epoch 105/200\n",
      "557/557 [==============================] - 0s - loss: 0.1023 - acc: 0.8905 - val_loss: 0.1612 - val_acc: 0.8992\n",
      "Epoch 106/200\n",
      "557/557 [==============================] - 0s - loss: 0.1097 - acc: 0.8977 - val_loss: 0.1801 - val_acc: 0.8992\n",
      "Epoch 107/200\n",
      "557/557 [==============================] - 0s - loss: 0.0939 - acc: 0.8995 - val_loss: 0.1620 - val_acc: 0.8908\n",
      "Epoch 108/200\n",
      "557/557 [==============================] - 0s - loss: 0.0871 - acc: 0.8959 - val_loss: 0.1900 - val_acc: 0.8824\n",
      "Epoch 109/200\n",
      "557/557 [==============================] - 0s - loss: 0.0946 - acc: 0.8995 - val_loss: 0.1685 - val_acc: 0.8908\n",
      "Epoch 110/200\n",
      "557/557 [==============================] - 0s - loss: 0.0944 - acc: 0.8833 - val_loss: 0.1367 - val_acc: 0.8992\n",
      "Epoch 111/200\n",
      "557/557 [==============================] - 0s - loss: 0.1155 - acc: 0.8833 - val_loss: 0.1496 - val_acc: 0.8824\n",
      "Epoch 112/200\n",
      "557/557 [==============================] - 0s - loss: 0.1495 - acc: 0.8851 - val_loss: 0.1629 - val_acc: 0.8908\n",
      "Epoch 113/200\n",
      "557/557 [==============================] - 0s - loss: 0.1312 - acc: 0.8815 - val_loss: 0.1616 - val_acc: 0.8824\n",
      "Epoch 114/200\n",
      "557/557 [==============================] - 1s - loss: 0.1036 - acc: 0.8905 - val_loss: 0.1304 - val_acc: 0.8992\n",
      "Epoch 115/200\n",
      "557/557 [==============================] - 0s - loss: 0.0953 - acc: 0.8977 - val_loss: 0.1328 - val_acc: 0.8992\n",
      "Epoch 116/200\n",
      "557/557 [==============================] - 0s - loss: 0.0899 - acc: 0.9210 - val_loss: 0.1390 - val_acc: 0.8992\n",
      "Epoch 117/200\n",
      "557/557 [==============================] - 0s - loss: 0.0916 - acc: 0.9174 - val_loss: 0.1249 - val_acc: 0.8992\n",
      "Epoch 118/200\n",
      "557/557 [==============================] - 0s - loss: 0.0973 - acc: 0.9192 - val_loss: 0.1592 - val_acc: 0.9412\n",
      "Epoch 119/200\n",
      "557/557 [==============================] - 1s - loss: 0.1247 - acc: 0.9084 - val_loss: 0.1377 - val_acc: 0.8908\n",
      "Epoch 120/200\n",
      "557/557 [==============================] - 1s - loss: 0.1413 - acc: 0.8923 - val_loss: 0.1396 - val_acc: 0.8992\n",
      "Epoch 121/200\n",
      "557/557 [==============================] - 0s - loss: 0.1448 - acc: 0.8833 - val_loss: 0.1361 - val_acc: 0.8992\n",
      "Epoch 122/200\n",
      "557/557 [==============================] - 0s - loss: 0.0861 - acc: 0.9013 - val_loss: 0.1336 - val_acc: 0.8908\n",
      "Epoch 123/200\n",
      "557/557 [==============================] - 0s - loss: 0.0804 - acc: 0.9138 - val_loss: 0.1323 - val_acc: 0.9076\n",
      "Epoch 124/200\n",
      "557/557 [==============================] - 0s - loss: 0.0865 - acc: 0.9013 - val_loss: 0.1402 - val_acc: 0.9160\n",
      "Epoch 125/200\n",
      "557/557 [==============================] - 0s - loss: 0.0832 - acc: 0.8995 - val_loss: 0.1429 - val_acc: 0.8992\n",
      "Epoch 126/200\n",
      "557/557 [==============================] - 0s - loss: 0.0848 - acc: 0.9031 - val_loss: 0.1362 - val_acc: 0.8992\n",
      "Epoch 127/200\n",
      "557/557 [==============================] - 0s - loss: 0.0740 - acc: 0.9066 - val_loss: 0.1412 - val_acc: 0.8992\n",
      "Epoch 128/200\n",
      "557/557 [==============================] - 0s - loss: 0.0807 - acc: 0.8923 - val_loss: 0.1932 - val_acc: 0.8739\n",
      "Epoch 129/200\n",
      "557/557 [==============================] - 0s - loss: 0.1693 - acc: 0.8797 - val_loss: 0.2198 - val_acc: 0.8824\n",
      "Epoch 130/200\n",
      "557/557 [==============================] - 0s - loss: 0.1608 - acc: 0.8600 - val_loss: 0.1722 - val_acc: 0.8824\n",
      "Epoch 131/200\n",
      "557/557 [==============================] - 0s - loss: 0.1851 - acc: 0.8618 - val_loss: 0.2302 - val_acc: 0.8908\n",
      "Epoch 132/200\n",
      "557/557 [==============================] - 0s - loss: 0.1438 - acc: 0.8761 - val_loss: 0.1539 - val_acc: 0.8908\n",
      "Epoch 133/200\n",
      "557/557 [==============================] - 0s - loss: 0.1347 - acc: 0.8797 - val_loss: 0.1373 - val_acc: 0.8908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa435aa5f50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, nb_epoch=200, shuffle=True, callbacks=[earlyStopping], \n",
    "          validation_data = (x_valid, y_valid), batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/121 [=======================>......] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2663965139630412, 0.82644626916932662]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121/121 [==============================] - 0s     \n",
      "[1 2 1 0 1 0 3 2 0 1 1 0 3 0 1 0 0 2 1 1 1 1 1 1 1 1 2 0 0 1 0 1 3 0 2 0 0\n",
      " 3 0 3 0 0 0 2 3 2 0 0 0 0 1 3 3 1 0 0 0 1 3 0 3 0 0 1 1 1 0 0 3 3 1 1 1 0\n",
      " 1 0 0 0 1 2 0 0 0 0 0 0 0 0 0 1 2 0 0 1 1 0 0 3 2 0 1 2 1 3 3 0 2 1 0 1 2\n",
      " 3 3 3 1 0 0 2 2 2 2]\n",
      " 96/121 [======================>.......] - ETA: 0s                precision    recall  f1-score   support\n",
      "\n",
      "class 0(0back)       0.96      0.78      0.86        63\n",
      "class 1(1back)       0.57      0.87      0.69        23\n",
      "class 2(2back)       0.88      0.94      0.91        16\n",
      " class3(3back)       0.89      0.84      0.86        19\n",
      "\n",
      "   avg / total       0.87      0.83      0.83       121\n",
      "\n",
      "[[49 14  0  0]\n",
      " [ 2 20  0  1]\n",
      " [ 0  0 15  1]\n",
      " [ 0  1  2 16]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "y_pred = model.predict_classes(x_test)\n",
    "print(y_pred)\n",
    "\n",
    "p=model.predict_proba(x_test)\n",
    "\n",
    "target_names = ['class 0(0back)', 'class 1(1back)', 'class 2(2back)', 'class3(3back)']\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"476pt\" viewBox=\"0.00 0.00 302.00 476.00\" width=\"302pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 472)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-472 298,-472 298,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140343399317712 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140343399317712</title>\n",
       "<polygon fill=\"none\" points=\"0,-421 0,-467 294,-467 294,-421 0,-421\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"78\" y=\"-440.3\">lstm_input_1: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"156,-421 156,-467 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.5\" y=\"-451.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"156,-444 211,-444 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.5\" y=\"-428.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"211,-421 211,-467 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252.5\" y=\"-451.8\">(None, 8, 6)</text>\n",
       "<polyline fill=\"none\" points=\"211,-444 294,-444 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252.5\" y=\"-428.8\">(None, 8, 6)</text>\n",
       "</g>\n",
       "<!-- 140343399317776 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140343399317776</title>\n",
       "<polygon fill=\"none\" points=\"25.5,-337 25.5,-383 268.5,-383 268.5,-337 25.5,-337\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74.5\" y=\"-356.3\">lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"123.5,-337 123.5,-383 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"151\" y=\"-367.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"123.5,-360 178.5,-360 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"151\" y=\"-344.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"178.5,-337 178.5,-383 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223.5\" y=\"-367.8\">(None, 8, 6)</text>\n",
       "<polyline fill=\"none\" points=\"178.5,-360 268.5,-360 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223.5\" y=\"-344.8\">(None, 8, 64)</text>\n",
       "</g>\n",
       "<!-- 140343399317712&#45;&gt;140343399317776 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140343399317712-&gt;140343399317776</title>\n",
       "<path d=\"M147,-420.593C147,-412.118 147,-402.297 147,-393.104\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"150.5,-393.096 147,-383.096 143.5,-393.096 150.5,-393.096\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140343399317584 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140343399317584</title>\n",
       "<polygon fill=\"none\" points=\"12,-253 12,-299 282,-299 282,-253 12,-253\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74.5\" y=\"-272.3\">dropout_1: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"137,-253 137,-299 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164.5\" y=\"-283.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"137,-276 192,-276 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164.5\" y=\"-260.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"192,-253 192,-299 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"237\" y=\"-283.8\">(None, 8, 64)</text>\n",
       "<polyline fill=\"none\" points=\"192,-276 282,-276 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"237\" y=\"-260.8\">(None, 8, 64)</text>\n",
       "</g>\n",
       "<!-- 140343399317776&#45;&gt;140343399317584 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140343399317776-&gt;140343399317584</title>\n",
       "<path d=\"M147,-336.593C147,-328.118 147,-318.297 147,-309.104\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"150.5,-309.096 147,-299.096 143.5,-309.096 150.5,-309.096\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140343389117200 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140343389117200</title>\n",
       "<polygon fill=\"none\" points=\"25.5,-169 25.5,-215 268.5,-215 268.5,-169 25.5,-169\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74.5\" y=\"-188.3\">lstm_2: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"123.5,-169 123.5,-215 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"151\" y=\"-199.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"123.5,-192 178.5,-192 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"151\" y=\"-176.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"178.5,-169 178.5,-215 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223.5\" y=\"-199.8\">(None, 8, 64)</text>\n",
       "<polyline fill=\"none\" points=\"178.5,-192 268.5,-192 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223.5\" y=\"-176.8\">(None, 32)</text>\n",
       "</g>\n",
       "<!-- 140343399317584&#45;&gt;140343389117200 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140343399317584-&gt;140343389117200</title>\n",
       "<path d=\"M147,-252.593C147,-244.118 147,-234.297 147,-225.104\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"150.5,-225.096 147,-215.096 143.5,-225.096 150.5,-225.096\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140343388106192 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140343388106192</title>\n",
       "<polygon fill=\"none\" points=\"19,-85 19,-131 275,-131 275,-85 19,-85\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-104.3\">dropout_2: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"144,-85 144,-131 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"171.5\" y=\"-115.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"144,-108 199,-108 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"171.5\" y=\"-92.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"199,-85 199,-131 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"237\" y=\"-115.8\">(None, 32)</text>\n",
       "<polyline fill=\"none\" points=\"199,-108 275,-108 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"237\" y=\"-92.8\">(None, 32)</text>\n",
       "</g>\n",
       "<!-- 140343389117200&#45;&gt;140343388106192 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140343389117200-&gt;140343388106192</title>\n",
       "<path d=\"M147,-168.593C147,-160.118 147,-150.297 147,-141.104\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"150.5,-141.096 147,-131.096 143.5,-141.096 150.5,-141.096\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140343387297104 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140343387297104</title>\n",
       "<polygon fill=\"none\" points=\"30.5,-1 30.5,-47 263.5,-47 263.5,-1 30.5,-1\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-20.3\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"132.5,-1 132.5,-47 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"160\" y=\"-31.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"132.5,-24 187.5,-24 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"160\" y=\"-8.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"187.5,-1 187.5,-47 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225.5\" y=\"-31.8\">(None, 32)</text>\n",
       "<polyline fill=\"none\" points=\"187.5,-24 263.5,-24 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225.5\" y=\"-8.8\">(None, 4)</text>\n",
       "</g>\n",
       "<!-- 140343388106192&#45;&gt;140343387297104 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140343388106192-&gt;140343387297104</title>\n",
       "<path d=\"M147,-84.5931C147,-76.1177 147,-66.2974 147,-57.104\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"150.5,-57.0958 147,-47.0959 143.5,-57.0959 150.5,-57.0958\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display, SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "# Show the model in ipython notebook\n",
    "figure = SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))\n",
    "display(figure)\n",
    "\n",
    "# Save the model as png file\n",
    "from keras.utils.visualize_util import plot\n",
    "plot(model, to_file='model_rnn.png', show_shapes=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
